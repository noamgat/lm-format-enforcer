{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LM Format Enforcer Integration with vLLM\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/noamgat/lm-format-enforcer/blob/main/samples/colab_vllm_integration.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "This notebook shows how you can integrate with the vLLM library. vLLM does not currently have an API for token filtering, so we have to do some monkey patching to expose the functionality.\n",
    "\n",
    "## Setting up the COLAB runtime (user action required)\n",
    "\n",
    "This colab-friendly notebook is targeted at demoing the enforcer on LLAMA2. It can run on a free GPU on Google Colab.\n",
    "Make sure that your runtime is set to GPU:\n",
    "\n",
    "Menu Bar -> Runtime -> Change runtime type -> T4 GPU (at the time of writing this notebook). [Guide here](https://www.codesansar.com/deep-learning/using-free-gpu-tpu-google-colab.htm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering huggingface credentials (user action required)\n",
    "\n",
    "We begin by installing the dependencies. This demo uses llama2, so you will have to create a free huggingface account, request access to the llama2 model, create an access token, and insert it when executing the next cell will request it.\n",
    "\n",
    "Links:\n",
    "\n",
    "- [Request access to llama model](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf). See the \"Access Llama 2 on Hugging Face\" section.\n",
    "- [Create huggingface access token](https://huggingface.co/settings/tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vllm lm-format-enforcer\n",
    "!huggingface-cli login\n",
    "\n",
    "# When running from source / developing the library, use this instead\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# import sys\n",
    "# import os\n",
    "# sys.path.append(os.path.abspath('..'))\n",
    "## os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a custom sampler that filters tokens\n",
    "\n",
    "We introduce a subclass of vLLM's ```SamplingParams``` that also accepts a token filtering function, with the same API as Huggingface Transformers \n",
    "\n",
    "```prefix_allowed_tokens_fn: Callable[[int, torch.Tensor], List[int]]```\n",
    "\n",
    "We then introduce the function ```_apply_allowed_token_filters()``` that applies the filter functions to the logits (sets them to negative infinity if not allowed) to requests that contain a filter function.\n",
    "\n",
    "We hope that in future releases of vLLM, this (or similar) will be part of vLLM's ```Sampler``` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noamgat/mambaforge/envs/vllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-10-18 21:01:08,225\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2023-10-18 21:01:08,604\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import vllm\n",
    "import torch\n",
    "from typing import List, Callable, Optional\n",
    "from vllm.sampling_params import SamplingParams\n",
    "from vllm.model_executor.input_metadata import InputMetadata\n",
    "\n",
    "class SamplingParamsWithFilterFunction(SamplingParams):\n",
    "    logits_allowed_tokens_filter_function: Optional[Callable[[int, torch.Tensor], List[int]]]\n",
    "\n",
    "def _apply_allowed_token_filters(logits: torch.Tensor, \n",
    "                                 input_metadata: InputMetadata) -> torch.Tensor:\n",
    "    num_seqs, vocab_size = logits.shape\n",
    "    logits_row_idx = 0\n",
    "    for seq_ids, sampling_params in input_metadata.seq_groups:\n",
    "        if isinstance(sampling_params, SamplingParamsWithFilterFunction):\n",
    "            filter_function = sampling_params.logits_allowed_tokens_filter_function\n",
    "        else:\n",
    "            filter_function = None\n",
    "        for seq_id in seq_ids:\n",
    "            if filter_function is not None:\n",
    "                output_token_ids = input_metadata.seq_data[seq_id].output_token_ids\n",
    "                output_token_tensor = torch.tensor(output_token_ids, dtype=torch.long)\n",
    "                allowed_tokens = filter_function(logits_row_idx, output_token_tensor)\n",
    "                logits_add_factor = torch.zeros(vocab_size, dtype=logits.dtype, device=logits.device)\n",
    "                logits_add_factor[:] = float('-inf')\n",
    "                logits_add_factor[allowed_tokens] = 0\n",
    "                logits[logits_row_idx] += logits_add_factor\n",
    "            logits_row_idx += 1\n",
    "    assert logits_row_idx == num_seqs\n",
    "    return logits \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to integrate this function with the ```Sampler``` class, we have to change its ```forward()``` function to call it. Since we are not modifying vLLM itself, we will do this with monkey patching. \n",
    "\n",
    "Other than the line \n",
    "```\n",
    "logits = _apply_allowed_token_filters(logits, input_metadata)\n",
    "```\n",
    "this is a 100% copy of the original ```Sampler.forward()``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.model_executor.layers.sampler import SamplerOutput, _prune_hidden_states, _get_logits, _get_output_tokens, _get_penalties, _apply_penalties, _get_temperatures, _get_top_p_top_k, _apply_top_p_top_k, _sample, _get_logprobs, _build_sampler_output, _SAMPLING_EPS\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "# @wrapt.patch_function_wrapper(vllm.model_executor.layers.Sampler, 'forward')\n",
    "def patched_forward(\n",
    "        self,\n",
    "        embedding: torch.Tensor,\n",
    "        hidden_states: torch.Tensor,\n",
    "        input_metadata: InputMetadata,\n",
    "        embedding_bias: Optional[torch.Tensor] = None,\n",
    "    ) -> SamplerOutput:\n",
    "        # Get the hidden states that we use for sampling.\n",
    "        hidden_states = _prune_hidden_states(hidden_states, input_metadata)\n",
    "\n",
    "        # Get the logits for the next tokens.\n",
    "        logits = _get_logits(hidden_states, embedding, embedding_bias,\n",
    "                             self.vocab_size)\n",
    "        \n",
    "        # Apply presence and frequency penalties.\n",
    "        output_tokens = _get_output_tokens(input_metadata)\n",
    "        assert len(output_tokens) == logits.shape[0]\n",
    "        presence_penalties, frequency_penalties = _get_penalties(\n",
    "            input_metadata)\n",
    "        assert len(presence_penalties) == logits.shape[0]\n",
    "        assert len(frequency_penalties) == logits.shape[0]\n",
    "        logits = _apply_penalties(logits, output_tokens, presence_penalties,\n",
    "                                  frequency_penalties)\n",
    "        \n",
    "        ### LM FORMAT ENFORCER MONKEY PATCH START\n",
    "        logits = _apply_allowed_token_filters(logits, input_metadata)\n",
    "        ### LM FORMAT ENFORCER MONKEY PATCH END\n",
    "\n",
    "        # Apply temperature scaling.\n",
    "        temperatures = _get_temperatures(input_metadata)\n",
    "        assert len(temperatures) == logits.shape[0]\n",
    "        if any(t != 1.0 for t in temperatures):\n",
    "            t = torch.tensor(temperatures,\n",
    "                             dtype=logits.dtype,\n",
    "                             device=logits.device)\n",
    "            # Use in-place division to avoid creating a new tensor.\n",
    "            logits.div_(t.unsqueeze(dim=1))\n",
    "\n",
    "        # Apply top-p and top-k truncation.\n",
    "        top_ps, top_ks = _get_top_p_top_k(input_metadata, self.vocab_size)\n",
    "        assert len(top_ps) == len(top_ks) == logits.shape[0]\n",
    "        do_top_p = any(p < 1.0 - _SAMPLING_EPS for p in top_ps)\n",
    "        do_top_k = any(k != self.vocab_size for k in top_ks)\n",
    "        if do_top_p or do_top_k:\n",
    "            logits = _apply_top_p_top_k(logits, top_ps, top_ks)\n",
    "\n",
    "        # We use float32 for probabilities and log probabilities.\n",
    "        # Compute the probabilities.\n",
    "        probs = torch.softmax(logits, dim=-1, dtype=torch.float)\n",
    "        # Compute the log probabilities.\n",
    "        # Use log_softmax to ensure numerical stability.\n",
    "        logprobs = torch.log_softmax(logits, dim=-1, dtype=torch.float)\n",
    "\n",
    "        # Sample the next tokens.\n",
    "        sample_results = _sample(probs, logprobs, input_metadata)\n",
    "        # Get the logprobs query results.\n",
    "        prompt_logprobs, sample_logprobs = _get_logprobs(\n",
    "            logprobs, input_metadata, sample_results)\n",
    "        return _build_sampler_output(sample_results, input_metadata,\n",
    "                                     prompt_logprobs, sample_logprobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the model, as is normally done with vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-18 21:01:10 llm_engine.py:72] Initializing an LLM engine with config: model='meta-llama/Llama-2-7b-chat-hf', tokenizer='meta-llama/Llama-2-7b-chat-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)\n",
      "INFO 10-18 21:01:10 tokenizer.py:31] For some LLaMA V1 models, initializing the fast tokenizer may take a long time. To reduce the initialization time, consider using 'hf-internal-testing/llama-tokenizer' instead of the original tokenizer.\n",
      "INFO 10-18 21:07:48 llm_engine.py:207] # GPU blocks: 1091, # CPU blocks: 512\n",
      "WARNING 10-18 21:07:48 cache_engine.py:96] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n"
     ]
    }
   ],
   "source": [
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "llm = vllm.LLM(model=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous cell executed successfully, you have propertly set up your Colab runtime and huggingface account!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few helper functions to make display nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def display_header(text):\n",
    "    display(Markdown(f'**{text}**'))\n",
    "\n",
    "def display_content(text):\n",
    "    display(Markdown(f'```\\n{text}\\n```'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the prompt for the specific language model\n",
    "\n",
    "We set up the prompting style according to the demo at https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat/blob/main/app.py . We simplify the implementation a bit as we don't need chat history for this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\\n",
    "\"\"\"\n",
    "\n",
    "def get_prompt(message: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "    texts = [f'<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n']\n",
    "    # The first user input is _not_ stripped\n",
    "    do_strip = False\n",
    "    message = message.strip() if do_strip else message\n",
    "    texts.append(f'{message} [/INST]')\n",
    "    return ''.join(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activating the monkey patch and creating the generation function\n",
    "\n",
    "We monkey-patch the ```Sampler``` class with our custom ```forward()``` method, using ```unittest.mock```.\n",
    "\n",
    "We use our sampling params in order to sent the specific filter function with the request. Different requests can have different format enforcers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmformatenforcer import build_transformers_prefix_allowed_tokens_fn, CharacterLevelParser\n",
    "from unittest import mock\n",
    "\n",
    "DEFAULT_MAX_NEW_TOKENS = 100\n",
    "\n",
    "def vllm_with_character_level_parser(llm: vllm.LLM, prompt: str, parser: Optional[CharacterLevelParser] = None) -> str:\n",
    "    with mock.patch.object(vllm.model_executor.layers.sampler.Sampler, 'forward', patched_forward):\n",
    "        prefix_function = build_transformers_prefix_allowed_tokens_fn(llm.get_tokenizer(), parser) if parser else None\n",
    "        sampling_params = SamplingParamsWithFilterFunction()\n",
    "        sampling_params.max_tokens = DEFAULT_MAX_NEW_TOKENS\n",
    "        sampling_params.logits_allowed_tokens_filter_function = prefix_function\n",
    "        result = llm.generate(prompt, sampling_params=sampling_params)\n",
    "        return result[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vLLM + JSON Use case\n",
    "\n",
    "Now we demonstrate using ```JsonSchemaParser```. We create a pydantic model, generate the schema from it, and use that to enforce the format.\n",
    "The output will always be in a format that can be parsed by the parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "<s>[INST] <<SYS>>\n",
       "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
       "\n",
       "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
       "<</SYS>>\n",
       "\n",
       "Please give me information about Michael Jordan. You MUST answer using the following json schema: {\"title\": \"AnswerFormat\", \"type\": \"object\", \"properties\": {\"first_name\": {\"title\": \"First Name\", \"type\": \"string\"}, \"last_name\": {\"title\": \"Last Name\", \"type\": \"string\"}, \"year_of_birth\": {\"title\": \"Year Of Birth\", \"type\": \"integer\"}, \"num_seasons_in_nba\": {\"title\": \"Num Seasons In Nba\", \"type\": \"integer\"}}, \"required\": [\"first_name\", \"last_name\", \"year_of_birth\", \"num_seasons_in_nba\"]} [/INST]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Answer, With json schema enforcing:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "  { \"first_name\": \"Michael\", \"last_name\": \"Jordan\", \"year_of_birth\": 1963, \"num_seasons_in_nba\": 15 }\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Answer, Without json schema enforcing:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "  Of course, I'd be happy to provide you with information about Michael Jordan! Here is the information in the format you requested:\n",
       "{\n",
       "\"title\": \"AnswerFormat\",\n",
       "\"type\": \"object\",\n",
       "\"properties\": {\n",
       "\"first_name\": {\n",
       "\"title\": \"First Name\",\n",
       "\"type\": \"string\",\n",
       "\"description\": \"Michael Jordan's first name.\"\n",
       "},\n",
       "\"last_name\": {\n",
       "\n",
       "\"title\": \"Last\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lmformatenforcer import JsonSchemaParser\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from typing import List\n",
    "\n",
    "class AnswerFormat(BaseModel):\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    year_of_birth: int\n",
    "    num_seasons_in_nba: int\n",
    "\n",
    "question = 'Please give me information about Michael Jordan. You MUST answer using the following json schema: '\n",
    "question_with_schema = f'{question}{AnswerFormat.schema_json()}'\n",
    "prompt = get_prompt(question_with_schema)\n",
    "\n",
    "display_header(\"Prompt:\")\n",
    "display_content(prompt)\n",
    "\n",
    "display_header(\"Answer, With json schema enforcing:\")\n",
    "\n",
    "result = vllm_with_character_level_parser(llm, prompt, JsonSchemaParser(AnswerFormat.schema()))\n",
    "display_content(result)\n",
    "\n",
    "display_header(\"Answer, Without json schema enforcing:\")\n",
    "result = vllm_with_character_level_parser(llm, prompt, None)\n",
    "display_content(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the enforced output matches the required schema, while the unenforced does not. We have successfully integrated with vLLM!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmformatenforcer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
