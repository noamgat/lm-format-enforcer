{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LM Format Enforcer Integration with LlamaIndex\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/noamgat/lm-format-enforcer/blob/main/samples/colab_llamacpppython_integration.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "This notebook shows how you can integrate with the [LlamaIndex](https://github.com/run-llama/llama_index) library. Since LlamaIndex abstracts the underlying LLM but opens an interface to pass parameters to it, we will use our existing integrations with `transformers` and `llama-cpp-python` to integrate with LlamaIndex.\n",
    "\n",
    "### Setting up the COLAB runtime (user action required)\n",
    "\n",
    "This colab-friendly notebook is targeted at demoing the enforcer on LLAMA2. It can run on a free GPU on Google Colab.\n",
    "Make sure that your runtime is set to GPU:\n",
    "\n",
    "Menu Bar -> Runtime -> Change runtime type -> T4 GPU (at the time of writing this notebook). [Guide here](https://www.codesansar.com/deep-learning/using-free-gpu-tpu-google-colab.htm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing dependencies\n",
    "\n",
    "We begin by installing the dependencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index lm-format-enforcer torch transformers llama-cpp-python accelerate bitsandbytes cpm_kernels\n",
    "\n",
    "# When running from source / developing the library, use this instead\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# import sys\n",
    "# import os\n",
    "# sys.path.append(os.path.abspath('..'))\n",
    "## os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few helper functions to make display nice and have our prompting ready. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def display_header(text):\n",
    "    display(Markdown(f'**{text}**'))\n",
    "\n",
    "def display_content(text):\n",
    "    display(Markdown(f'```\\n{text}\\n```'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing our prompt and target output format\n",
    "\n",
    "We set up the prompting style according to the [Llama2 demo](https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat/blob/main/app.py). We simplify the implementation a bit as we don't need chat history for this demo.\n",
    "We use JSON Schema output for this example, but regex output is also available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_618202/3203385482.py:16: PydanticDeprecatedSince20: The `schema_json` method is deprecated; use `model_json_schema` and json.dumps instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.4/migration/\n",
      "  question_with_schema = f'{question}{AnswerFormat.schema_json()}'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pydantic import BaseModel\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\\n",
    "\"\"\"\n",
    "\n",
    "def get_prompt(message: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "    return f'<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{message} [/INST]'\n",
    "\n",
    "class AnswerFormat(BaseModel):\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    year_of_birth: int\n",
    "    num_seasons_in_nba: int\n",
    "\n",
    "question = 'Please give me information about Michael Jordan. You MUST answer using the following json schema: '\n",
    "question_with_schema = f'{question}{AnswerFormat.schema_json()}'\n",
    "prompt = get_prompt(question_with_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaIndex + HuggingFace Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demo uses llama2, so you will have to create a free huggingface account, request access to the llama2 model, create an access token, and insert it when executing the next cell will request it.\n",
    "\n",
    "Links:\n",
    "\n",
    "- [Request access to llama model](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf). See the \"Access Llama 2 on Hugging Face\" section.\n",
    "- [Create huggingface access token](https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model\n",
    "\n",
    "We load the model directly using transformers API in order to pass precise quantization parameters to it. Afterwards we initialize the LlamaIndex `HuggingFaceLLM` from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noamgat/mambaforge/envs/commentranker/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 614/614 [00:00<00:00, 2.44MB/s]\n",
      "Downloading shards: 100%|██████████| 2/2 [00:00<00:00,  3.15it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [06:53<00:00, 206.59s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "\n",
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "device = 'cuda'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "    config.pretraining_tp = 1\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        config=config,\n",
    "        torch_dtype=torch.float16,\n",
    "        load_in_8bit=True,\n",
    "        device_map='auto'\n",
    "    )\n",
    "else:\n",
    "    raise Exception('GPU not available')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    # Required for batching example\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id \n",
    "\n",
    "llm_huggingface = HuggingFaceLLM(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous cell executed successfully, you have propertly set up your Colab runtime and huggingface account!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating LM Format Enforcer and generating JSON Schema\n",
    "\n",
    "Now we demonstrate using ```JsonSchemaParser```. The output will always be in a format that can be parsed by the parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "<s>[INST] <<SYS>>\n",
       "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
       "\n",
       "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
       "<</SYS>>\n",
       "\n",
       "Please give me information about Michael Jordan. You MUST answer using the following json schema: {\"properties\": {\"first_name\": {\"title\": \"First Name\", \"type\": \"string\"}, \"last_name\": {\"title\": \"Last Name\", \"type\": \"string\"}, \"year_of_birth\": {\"title\": \"Year Of Birth\", \"type\": \"integer\"}, \"num_seasons_in_nba\": {\"title\": \"Num Seasons In Nba\", \"type\": \"integer\"}}, \"required\": [\"first_name\", \"last_name\", \"year_of_birth\", \"num_seasons_in_nba\"], \"title\": \"AnswerFormat\", \"type\": \"object\"} [/INST]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Answer, Without json schema enforcing:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "  Of course! I'd be happy to help you with information about Michael Jordan. Here is the information in the format you requested:\n",
       "{\n",
       "\"title\": \"AnswerFormat\",\n",
       "\"type\": \"object\",\n",
       "\"properties\": {\n",
       "\"first_name\": {\"title\": \"First Name\", \"type\": \"string\"},\n",
       "\"last_name\": {\"title\": \"Last Name\", \"type\": \"string\"},\n",
       "\"year_of_birth\": {\"title\":\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Answer, With json schema enforcing:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_618202/2786092233.py:25: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.4/migration/\n",
      "  result = llamaindex_huggingface_lm_format_enforcer(llm_huggingface, prompt, JsonSchemaParser(AnswerFormat.schema()))\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       " { \"first_name\": \"Michael\", \"last_name\": \"Jordan\", \"year_of_birth\": 1963, \"num_seasons_in_nba\": 15 }\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Optional\n",
    "from lmformatenforcer import CharacterLevelParser, JsonSchemaParser\n",
    "from lmformatenforcer.integrations.transformers import build_transformers_prefix_allowed_tokens_fn\n",
    "\n",
    "def llamaindex_huggingface_lm_format_enforcer(llm: HuggingFaceLLM, prompt: str, character_level_parser: Optional[CharacterLevelParser]) -> str:\n",
    "    prefix_allowed_tokens_fn = None\n",
    "    if character_level_parser:\n",
    "        prefix_allowed_tokens_fn = build_transformers_prefix_allowed_tokens_fn(llm._tokenizer, character_level_parser)\n",
    "    \n",
    "    # If changing the character level parser each call, inject it before calling complete. If its the same format\n",
    "    # each time, you can set it once after creating the HuggingFaceLLM model\n",
    "    llm.generate_kwargs['prefix_allowed_tokens_fn'] = prefix_allowed_tokens_fn\n",
    "    output = llm.complete(prompt)\n",
    "    text: str = output.text\n",
    "    return text\n",
    "\n",
    "display_header(\"Prompt:\")\n",
    "display_content(prompt)\n",
    "\n",
    "display_header(\"Answer, Without json schema enforcing:\")\n",
    "result = llamaindex_huggingface_lm_format_enforcer(llm_huggingface, prompt, None)\n",
    "display_content(result)\n",
    "\n",
    "display_header(\"Answer, With json schema enforcing:\")\n",
    "result = llamaindex_huggingface_lm_format_enforcer(llm_huggingface, prompt, JsonSchemaParser(AnswerFormat.schema()))\n",
    "display_content(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaIndex + LlamaCPP\n",
    "\n",
    "This demo uses [Llama2 gguf weights by TheBloke](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF). We will use huggingface hub to download the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /home/noamgat/huggingface/hub/models--TheBloke--Llama-2-7b-Chat-GGUF/snapshots/191239b3e26b2882fb562ffccdd1cf0f65402adb/llama-2-7b-chat.Q5_K_M.gguf (version GGUF V2 (latest))\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import LlamaCPP\n",
    "from huggingface_hub import hf_hub_download\n",
    "downloaded_model_path = hf_hub_download(repo_id=\"TheBloke/Llama-2-7b-Chat-GGUF\", filename=\"llama-2-7b-chat.Q5_K_M.gguf\")\n",
    "llm_llamacpp = LlamaCPP(model_path=downloaded_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating LM Format Enforcer and generating JSON Schema\n",
    "\n",
    "Now we demonstrate using ```JsonSchemaParser```. The output will always be in a format that can be parsed by the parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_612108/4226464266.py:26: PydanticDeprecatedSince20: The `schema_json` method is deprecated; use `model_json_schema` and json.dumps instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.4/migration/\n",
      "  question_with_schema = f'{question}{AnswerFormat.schema_json()}'\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "<s>[INST] <<SYS>>\n",
       "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
       "\n",
       "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
       "<</SYS>>\n",
       "\n",
       "Please give me information about Michael Jordan. You MUST answer using the following json schema: {\"properties\": {\"first_name\": {\"title\": \"First Name\", \"type\": \"string\"}, \"last_name\": {\"title\": \"Last Name\", \"type\": \"string\"}, \"year_of_birth\": {\"title\": \"Year Of Birth\", \"type\": \"integer\"}, \"num_seasons_in_nba\": {\"title\": \"Num Seasons In Nba\", \"type\": \"integer\"}}, \"required\": [\"first_name\", \"last_name\", \"year_of_birth\", \"num_seasons_in_nba\"], \"title\": \"AnswerFormat\", \"type\": \"object\"} [/INST]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Answer, Without json schema enforcing:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 18009.19 ms\n",
      "llama_print_timings:      sample time =    33.98 ms /    99 runs   (    0.34 ms per token,  2913.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  9239.67 ms /    99 runs   (   93.33 ms per token,    10.71 tokens per second)\n",
      "llama_print_timings:       total time =  9415.06 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "  Of course! I'd be happy to help you with information about Michael Jordan. Here is the information in the format you requested:\n",
       "{\n",
       "\"title\": \"AnswerFormat\",\n",
       "\"type\": \"object\",\n",
       "\"properties\": {\n",
       "\"first_name\": {\"title\": \"First Name\", \"type\": \"string\"},\n",
       "\"last_name\": {\"title\": \"Last Name\", \"type\": \"string\"},\n",
       "\"year_of_birth\": {\"title\":\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Answer, With json schema enforcing:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_612108/4226464266.py:37: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.4/migration/\n",
      "  result = llamaindex_llamacpp_lm_format_enforcer(llm, prompt, JsonSchemaParser(AnswerFormat.schema()))\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 18009.19 ms\n",
      "llama_print_timings:      sample time =    17.44 ms /    53 runs   (    0.33 ms per token,  3038.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4980.95 ms /    53 runs   (   93.98 ms per token,    10.64 tokens per second)\n",
      "llama_print_timings:       total time =  5452.10 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "  { \"first_name\": \"Michael\", \"last_name\": \"Jordan\", \"year_of_birth\": 1963, \"num_seasons_in_nba\": 15 }\n",
       "\n",
       "\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Optional\n",
    "from llama_cpp import LogitsProcessorList\n",
    "from lmformatenforcer import CharacterLevelParser, JsonSchemaParser\n",
    "from lmformatenforcer.integrations.llamacpp import build_llamacpp_logits_processor\n",
    "\n",
    "def llamaindex_llamacpp_lm_format_enforcer(llm: LlamaCPP, prompt: str, character_level_parser: Optional[CharacterLevelParser]) -> str:\n",
    "    logits_processors: Optional[LogitsProcessorList] = None\n",
    "    if character_level_parser:\n",
    "        logits_processors = LogitsProcessorList([build_llamacpp_logits_processor(llm._model, character_level_parser)])\n",
    "    \n",
    "    # If changing the character level parser each call, inject it before calling complete. If its the same format\n",
    "    # each time, you can set it once after creating the LlamaCPP model\n",
    "    llm.generate_kwargs['logits_processor'] = logits_processors\n",
    "    output = llm.complete(prompt)\n",
    "    text: str = output.text\n",
    "    return text\n",
    "\n",
    "display_header(\"Prompt:\")\n",
    "display_content(prompt)\n",
    "\n",
    "display_header(\"Answer, Without json schema enforcing:\")\n",
    "result = llamaindex_llamacpp_lm_format_enforcer(llm_llamacpp, prompt, None)\n",
    "display_content(result)\n",
    "\n",
    "display_header(\"Answer, With json schema enforcing:\")\n",
    "result = llamaindex_llamacpp_lm_format_enforcer(llm_llamacpp, prompt, JsonSchemaParser(AnswerFormat.schema()))\n",
    "display_content(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the enforced output matches the required schema, while the unenforced does not. We have successfully integrated with llama.cpp!\n",
    "\n",
    "Ending note - the last cell probably took quite a long time to run. This is due to this notebook using CPU inference with `llamacpp`. LM Format Enforcer's runtime footprint is negligible compared to the model's runtime."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmformatenforcer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
